{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "#from stockfish import Stockfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "  load_in_4bit=True,\n",
    "  bnb_4bit_use_double_quant=True,\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "  bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 587/587 [00:00<00:00, 4.03MB/s]\n",
      "Downloading (…)fetensors.index.json: 100%|██████████| 33.4k/33.4k [00:00<00:00, 18.2MB/s]\n",
      "Downloading (…)of-00003.safetensors: 100%|██████████| 9.95G/9.95G [06:38<00:00, 24.9MB/s]\n",
      "Downloading (…)of-00003.safetensors: 100%|██████████| 9.90G/9.90G [06:58<00:00, 23.7MB/s]\n",
      "Downloading (…)of-00003.safetensors: 100%|██████████| 6.18G/6.18G [04:35<00:00, 22.5MB/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [18:13<00:00, 364.35s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.46s/it]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 188/188 [00:00<00:00, 1.76MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<00:00, 15.2MB/s]\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 26.5MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 4.21MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 3.82MB/s]\n"
     ]
    }
   ],
   "source": [
    "#model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "#model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "#model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            quantization_config=bnb_config,\n",
    "                                            load_in_4bit=True,\n",
    "                                            use_cache=False, \n",
    "                                            device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    max_length=512,\n",
    "    temperature=0.00001,\n",
    "    #top_p=0.15,\n",
    "    #repetition_penalty=1\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_json_chat_history():\n",
    "    json_chat_history = {\"Opponent\": [], \"Carlus Magnusen\": []}\n",
    "\n",
    "    return json_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chat_history(message, json_chat_history, bot=3):\n",
    "\n",
    "    if bot == 0:\n",
    "        json_chat_history[\"Opponent\"].append(message)\n",
    "    elif bot == 1:\n",
    "        json_chat_history[\"Carlus Magnusen\"].append(message)\n",
    "\n",
    "    formatted_dialogue_list = []\n",
    "\n",
    "    for i in range(len(json_chat_history[\"Opponent\"])):\n",
    "        formatted_dialogue_list.append(f'{json_chat_history[\"Opponent\"][i]}')\n",
    "        if i < len(json_chat_history[\"Carlus Magnusen\"]):\n",
    "            formatted_dialogue_list.append(f'{json_chat_history[\"Carlus Magnusen\"][i]}')\n",
    "\n",
    "        \n",
    "    formatted_chat_history = \"\"\n",
    "    for i, formatted_dialogue in enumerate(formatted_dialogue_list):\n",
    "        if i % 2 == 0:\n",
    "            formatted_chat_history = formatted_chat_history + formatted_dialogue + \" [/INST]\"\n",
    "        else:\n",
    "            formatted_chat_history = \" \" + formatted_chat_history + formatted_dialogue + \"</s><s>[INST] \"\n",
    "\n",
    "    #formatted_chat_history = '\\n'.join(formatted_dialogue_list)\n",
    "\n",
    "    return json_chat_history, formatted_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_json(text):\n",
    "    json_string = re.search(r'\\{.*\\}', text).group()\n",
    "    loaded_json = json.loads(json_string)\n",
    "    _, json_value = list(loaded_json.items())[0]\n",
    "    return json_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(prompt, formatted_chat_history, tool, best_move=None):\n",
    "\n",
    "#     template = f\"\"\"[INST] <<SYS>>\n",
    "# You are a helpful, respectful and honest assistant named Carlus Magnusen.\n",
    "# Act in a professional manner and keep your answers short.\n",
    "# Bellow you will find the best chess move, you should tell your opponent that this is the move you play.\n",
    "# Best chess move: e6\n",
    "# <</SYS>>\n",
    "# {formatted_chat_history}[/INST] \n",
    "# \"\"\"\n",
    "\n",
    "    if tool == \"classifier\":\n",
    "        system_message = \"\"\"Your objective is to classify the user input.\n",
    "There are two possible calssifications: [chess move, no chess move]\n",
    "If the user is playing a chess move against you, your classification should be \"chess move\" even if it is an ilegal move.\n",
    "If the user is not playing a chess move against you, your classification should be \"no chess move\".\n",
    "your answer must be as short as possible and have a json in the following format:\n",
    "{\"classification\": \"<<classification you chose>>\"}\"\"\"\n",
    "\n",
    "        full_prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]\"\"\"\n",
    "        \n",
    "    if tool == \"chess_move_extractor\":\n",
    "        system_message = \"\"\"Your objective is to extract the chess move that the user played against you.\n",
    "your answer must be justified and have a json in the following format:\n",
    "{\"move\": \"<<chess move you extracted in chess algebraic notation>>\"}\"\"\"\n",
    "\n",
    "        full_prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]\"\"\"\n",
    "\n",
    "    if tool == \"chess_bot\":\n",
    "        system_message = f\"\"\"You are Carlus Magnusen and your purpose is to play the chess move {best_move}\n",
    "Do not explain the reasoning behind your move.\n",
    "Act in a professional manner and keep your answers as short as possible.\"\"\"\n",
    "\n",
    "        full_prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]\"\"\"\n",
    "\n",
    "        full_prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "\n",
    "{formatted_chat_history}\"\"\"       \n",
    "        \n",
    "    if tool == \"chat_bot\":\n",
    "        system_message = f\"\"\"You are Carlus Magnusen.\n",
    "Act in a professional manner and keep your answers as short as possible.\"\"\"\n",
    "\n",
    "        full_prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]\"\"\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<s>[INST] <<SYS>>\n",
    "{your_system_message}\n",
    "<</SYS>>\n",
    "\n",
    "{user_message_1} [/INST] {model_reply_1}</s><s>[INST] {user_message_2} [/INST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I play e4 [/INST]  Very well, I shall play e5.\n",
      "\n",
      "My move is:\n",
      "\n",
      "1. e5\n",
      "\n",
      "Your move.</s><s>[INST] \n"
     ]
    }
   ],
   "source": [
    "print(formatted_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_chat_history = init_json_chat_history()\n",
    "chess_moves = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I play e4\"\n",
    "best_move = \"d5\"\n",
    "\n",
    "prompt = \"I take with my pawn on d5\"\n",
    "best_move = \"Qxd5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/anaconda3/envs/chess_bot/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>[INST] <<SYS>>\n",
      "Your objective is to classify the user input.\n",
      "There are two possible calssifications: [chess move, no chess move]\n",
      "If the user is playing a chess move against you, your classification should be \"chess move\" even if it is an ilegal move.\n",
      "If the user is not playing a chess move against you, your classification should be \"no chess move\".\n",
      "your answer must be as short as possible and have a json in the following format:\n",
      "{\"classification\": \"<<classification you chose>>\"}\n",
      "<</SYS>>\n",
      "\n",
      "I take with my pawn on d5 [/INST]\n",
      "\n",
      "\n",
      "classifier_output:   {\"classification\": \"chess move\"}\n",
      "\n",
      "\n",
      "CLASSIFICATION: chess move\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/anaconda3/envs/chess_bot/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>[INST] <<SYS>>\n",
      "Your objective is to extract the chess move that the user played against you.\n",
      "your answer must be justified and have a json in the following format:\n",
      "{\"move\": \"<<chess move you extracted in chess algebraic notation>>\"}\n",
      "<</SYS>>\n",
      "\n",
      "I take with my pawn on d5 [/INST]\n",
      "\n",
      "\n",
      "extractor_output:   Based on your move \"I take with my pawn on d5\", I can extract the chess move as follows:\n",
      "\n",
      "{\"move\": \"e4\"}\n",
      "\n",
      "Explanation:\n",
      "\n",
      "When you played \"I take with my pawn on d5\", you are capturing my pawn on d5 with your pawn on e4. This move is written in chess algebraic notation as \"e4\", which is the move that I have extracted.\n",
      "\n",
      "Therefore, the JSON output would be:\n",
      "\n",
      "{\"move\": \"e4\"}\n",
      "\n",
      "\n",
      "CHESS MOVE: e4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/anaconda3/envs/chess_bot/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>[INST] <<SYS>>\n",
      "You are Carlus Magnusen and your purpose is to play the chess move Qxd5\n",
      "Do not explain the reasoning behind your move.\n",
      "Act in a professional manner and keep your answers as short as possible.\n",
      "<</SYS>>\n",
      "\n",
      " I play e4 [/INST]  Very well. I respond with d5.</s><s>[INST] I take with my pawn on d5 [/INST]\n",
      "\n",
      "\n",
      "  Very well. I capture your pawn with my knight on d5.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/anaconda3/envs/chess_bot/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>[INST] <<SYS>>\n",
      "Your objective is to extract the chess move that the user played against you.\n",
      "your answer must be justified and have a json in the following format:\n",
      "{\"move\": \"<<chess move you extracted in chess algebraic notation>>\"}\n",
      "<</SYS>>\n",
      "\n",
      "  Very well. I capture your pawn with my knight on d5. [/INST]\n",
      "\n",
      "\n",
      "extractor_output:   Great! Based on your move, I can extract the chess move as follows:\n",
      "\n",
      "{\"move\": \"Nxd4\"}\n",
      "\n",
      "This move captures your pawn on d5 with my knight.\n",
      "\n",
      "\n",
      "CHESS MOVE: Nxd4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I take with my pawn on d5\"\n",
    "\n",
    "json_chat_history, formatted_chat_history = build_chat_history(prompt, json_chat_history, bot=0)\n",
    "\n",
    "full_prompt = build_prompt(prompt, formatted_chat_history, \"classifier\")\n",
    "classifier_output = local_llm(full_prompt)\n",
    "classification = retrieve_json(classifier_output)\n",
    "print(\"\\n\" + full_prompt + \"\\n\")\n",
    "print(\"\\nclassifier_output: \" + classifier_output + \"\\n\")\n",
    "print(\"\\nCLASSIFICATION: \" + classification + \"\\n\")\n",
    "\n",
    "if classification == 'chess move':\n",
    "    full_prompt = build_prompt(prompt, formatted_chat_history, \"chess_move_extractor\")\n",
    "    extractor_output = local_llm(full_prompt)\n",
    "    chess_move = retrieve_json(extractor_output)\n",
    "    chess_moves.append(chess_move)\n",
    "    print(\"\\n\" + full_prompt + \"\\n\")\n",
    "    print(\"\\nextractor_output: \" + extractor_output + \"\\n\")\n",
    "    print(\"\\nCHESS MOVE: \" + chess_move + \"\\n\")\n",
    "\n",
    "    # DO STOCKFISH HERE\n",
    "    best_move = \"Qxd5\"\n",
    "    \n",
    "    full_prompt = build_prompt(prompt, formatted_chat_history, \"chess_bot\", best_move)\n",
    "    output = local_llm(full_prompt)\n",
    "    json_chat_history, formatted_chat_history = build_chat_history(output, json_chat_history, bot=1)\n",
    "    print(\"\\n\" + full_prompt + \"\\n\")\n",
    "    print(\"\\n\" + output + \"\\n\")\n",
    "\n",
    "    full_prompt = build_prompt(output, formatted_chat_history, \"chess_move_extractor\")\n",
    "    extractor_output = local_llm(full_prompt)\n",
    "    chess_move = retrieve_json(extractor_output)\n",
    "    chess_moves.append(chess_move)\n",
    "    print(\"\\n\" + full_prompt + \"\\n\")\n",
    "    print(\"\\nextractor_output: \" + extractor_output + \"\\n\")\n",
    "    print(\"\\nCHESS MOVE: \" + chess_move + \"\\n\")\n",
    "\n",
    "\n",
    "if classification == 'no chess move':\n",
    "    full_prompt = build_prompt(prompt, formatted_chat_history, \"chat_bot\")\n",
    "    output = local_llm(full_prompt)\n",
    "    print(\"\\n\" + full_prompt + \"\\n\")\n",
    "    print(\"\\n\" + output + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e4', 'e5']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chess_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_test = \"\"\"<s>[INST] <<SYS>>\n",
    "Your objective is to extract the chess move that the user played against you.\n",
    "your answer must be jsutified and have a json in the following format:\n",
    "{\"move\": \"<<chess move you extracted in chess algebraic notation>>\"}\n",
    "<</SYS>>\n",
    "\n",
    "I take your pawn with my queen on d5 [/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/anaconda3/envs/chess_bot/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! I can extract the chess move you played against me. Based on your statement, you played the move \"Queen takes Pawn on d5\".\n",
      "\n",
      "Here's the JSON format response:\n",
      "\n",
      "{\"move\": \"Qxd5\"}\n"
     ]
    }
   ],
   "source": [
    "print(local_llm(string_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(prompt, chat_history):\n",
    "\n",
    "formated_chat_history = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "    template = \"\"\"[INST] <<SYS>>\n",
    "You are a Carlus Magnusen, the best chess player in the known universe and your purpose is to play chess with whoever wants to play with you. You are very confident and arrogant. You trash talk your opponents when you play chess. Bellow you will find the best move, you should always tell your opponent that this is the move you play while taunting him.\n",
    "Best Move: e6\n",
    "<</SYS>>\n",
    "{chat_history}\n",
    "Opponent: {prompt}[/INST]\n",
    "Carlus Magnusen: \n",
    "\"\"\"\n",
    "\n",
    "return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_config = {\n",
    "    \"Debug Log File\": \"\",\n",
    "    \"Contempt\": 0,\n",
    "    \"Min Split Depth\": 0,\n",
    "    \"Threads\": 8, # More threads will make the engine stronger, but should be kept at less than the number of logical processors on your computer.\n",
    "    \"Ponder\": \"false\",\n",
    "    \"Hash\": 2048, # Default size is 16 MB. It's recommended that you increase this value, but keep it as some power of 2. E.g., if you're fine using 2 GB of RAM, set Hash to 2048 (11th power of 2).\n",
    "    \"MultiPV\": 1,\n",
    "    \"Skill Level\": 20,\n",
    "    \"Move Overhead\": 10,\n",
    "    \"Minimum Thinking Time\": 20,\n",
    "    \"Slow Mover\": 100,\n",
    "    \"UCI_Chess960\": \"false\",\n",
    "    \"UCI_LimitStrength\": \"false\",\n",
    "    \"UCI_Elo\": 9999\n",
    "}\n",
    "\n",
    "Stockfish(path=\"/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/stockfish/stockfish-windows-x86-64-avx2.exe\",  depth=22, parameters=sf_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
