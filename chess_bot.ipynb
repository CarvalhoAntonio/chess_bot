{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/anaconda3/envs/chess_bot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from stockfish import Stockfish\n",
    "\n",
    "import torch\n",
    "\n",
    "from langchain import HuggingFacePipeline, LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "  load_in_4bit=True,\n",
    "  bnb_4bit_use_double_quant=True,\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "  bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 643/643 [00:00<00:00, 2.31MB/s]\n",
      "Downloading (…)fetensors.index.json: 100%|██████████| 23.9k/23.9k [00:00<00:00, 40.1MB/s]\n",
      "Downloading (…)of-00008.safetensors: 100%|██████████| 1.89G/1.89G [02:36<00:00, 12.1MB/s]\n",
      "Downloading (…)of-00008.safetensors: 100%|██████████| 1.95G/1.95G [02:28<00:00, 13.1MB/s]\n",
      "Downloading (…)of-00008.safetensors: 100%|██████████| 1.98G/1.98G [02:37<00:00, 12.6MB/s]\n",
      "Downloading (…)of-00008.safetensors: 100%|██████████| 1.95G/1.95G [02:23<00:00, 13.6MB/s]\n",
      "Downloading (…)of-00008.safetensors: 100%|██████████| 1.98G/1.98G [02:44<00:00, 12.0MB/s]\n",
      "Downloading (…)of-00008.safetensors: 100%|██████████| 1.95G/1.95G [02:40<00:00, 12.1MB/s]\n",
      "Downloading (…)of-00008.safetensors: 100%|██████████| 1.98G/1.98G [02:37<00:00, 12.6MB/s]\n",
      "Downloading (…)of-00008.safetensors: 100%|██████████| 816M/816M [01:05<00:00, 12.4MB/s]\n",
      "Downloading shards: 100%|██████████| 8/8 [19:16<00:00, 144.61s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:10<00:00,  1.37s/it]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 116/116 [00:00<00:00, 429kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 1.43k/1.43k [00:00<00:00, 5.31MB/s]\n",
      "Downloading tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 12.5MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.85MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 166kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 168/168 [00:00<00:00, 227kB/s]\n"
     ]
    }
   ],
   "source": [
    "#model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "#model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            quantization_config=bnb_config,\n",
    "                                            load_in_4bit=True,\n",
    "                                            use_cache=False, \n",
    "                                            device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    max_length=512,\n",
    "    temperature=0.1,\n",
    "    #top_p=0.15,\n",
    "    #repetition_penalty=1\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_json_chat_history():\n",
    "    json_chat_history = {\"Opponent\": [], \"Carlus Magnusen\": []}\n",
    "\n",
    "    return json_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chat_history(message, json_chat_history, bot):\n",
    "\n",
    "    if bot == 0:\n",
    "        json_chat_history[\"Opponent\"].append(message)\n",
    "    elif bot == 1:\n",
    "        json_chat_history[\"Carlus Magnusen\"].append(message)\n",
    "\n",
    "    formatted_dialogue = []\n",
    "\n",
    "    for i in range(len(json_chat_history[\"Opponent\"])):\n",
    "        formatted_dialogue.append(f'Opponent: {json_chat_history[\"Opponent\"][i]}')\n",
    "        if i < len(json_chat_history[\"Carlus Magnusen\"]):\n",
    "            formatted_dialogue.append(f'Carlus Magnusen: {json_chat_history[\"Carlus Magnusen\"][i]}')\n",
    "\n",
    "    formatted_chat_history = '\\n'.join(formatted_dialogue)\n",
    "\n",
    "    return json_chat_history, formatted_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(prompt, formatted_chat_history, tool):\n",
    "\n",
    "#     template = f\"\"\"[INST] <<SYS>>\n",
    "# You are a helpful, respectful and honest assistant named Carlus Magnusen.\n",
    "# Act in a professional manner and keep your answers short.\n",
    "# Bellow you will find the best chess move, you should tell your opponent that this is the move you play.\n",
    "# Best chess move: e6\n",
    "# <</SYS>>\n",
    "# {formatted_chat_history}[/INST] \n",
    "# \"\"\"\n",
    "\n",
    "#     template = f\"\"\"[INST] <<SYS>>\n",
    "# Your objective is to select a tool based on the user input.\n",
    "# There are two possible tools: [chat, chess]\n",
    "# Select the chess tool only if the user makes a chess move and select the chat tool otherwise\n",
    "# You must give your answer in the following json format:\n",
    "# {{\"tool\": '<<tool that you chose>>'}}\n",
    "# <</SYS>>\n",
    "# {prompt}[/INST]\n",
    "# \"\"\"\n",
    "\n",
    "    if tool == \"classifier\":\n",
    "        sys_message = \"\"\"[INST] <<SYS>>\n",
    "Your objective is to classify the user input.\n",
    "There are two possible calssifications: [chess move, no chess move]\n",
    "If you the user plays a chess move against you, your classification should be \"chess move\".\n",
    "If you the user does not play a chess move against you, your classification should be \"no chess move\".\n",
    "You must give your answer in the following json format:\n",
    "{\"classification\": \"<<classification you chose>>\"}\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "\n",
    "        full_prompt = sys_message + f\"\"\"{prompt}\"\"\" + \"[/INST]\\n\"\n",
    "\n",
    "        sys_message = \"\"\"<s>[INST] Your objective is to classify the user input.\n",
    "There are two possible calssifications: [chess move, no chess move]\n",
    "If you the user plays a chess move against you, your classification should be \"chess move\".\n",
    "If you the user does not play a chess move against you, your classification should be \"no chess move\".\n",
    "You must give your answer in the following json format:\n",
    "{\"classification\": \"<<classification you chose>>\"}\n",
    "\"\"\"\n",
    "\n",
    "        full_prompt = sys_message + f\"\"\"{prompt}\"\"\" + \" [/INST] \"\n",
    "\n",
    "    if tool == \"chat_bot\":\n",
    "        template = f\"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant named Carlus Magnusen.\n",
    "Act in a professional manner and keep your answers short.\n",
    "# <</SYS>>\n",
    "# {formatted_chat_history}[/INST]\n",
    "\"\"\"\n",
    "\n",
    "        full_prompt = sys_message + f\"\"\"{prompt}\"\"\" + \"[/INST]\\n\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_chat_history = init_json_chat_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"who are you\"\n",
    "json_chat_history, formatted_chat_history = build_chat_history(prompt, json_chat_history, bot=0)\n",
    "\n",
    "# prompt = \"im carlus\"\n",
    "# json_chat_history, formatted_chat_history = build_chat_history(prompt, json_chat_history, bot=1)\n",
    "\n",
    "# prompt = \"hi carlus\"\n",
    "# json_chat_history, formatted_chat_history = build_chat_history(prompt, json_chat_history, bot=0)\n",
    "\n",
    "full_prompt = build_prompt(prompt, formatted_chat_history, \"classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "Your objective is to classify the user input.\n",
      "There are two possible calssifications: [chess move, no chess move]\n",
      "If you the user plays a chess move against you, your classification should be \"chess move\".\n",
      "If you the user does not play a chess move against you, your classification should be \"no chess move\".\n",
      "You must give your answer in the following json format:\n",
      "{\"classification\": \"<<classification you chose>>\"}\n",
      "<</SYS>>\n",
      "who are you[/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/anaconda3/envs/chess_bot/lib/python3.11/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/home/antonio/anaconda3/envs/chess_bot/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output = local_llm(full_prompt)\n",
    "\n",
    "json_chat_history, formatted_chat_history = build_chat_history(output, json_chat_history, bot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tool\": \"chat\"}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/anaconda3/envs/chess_bot/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "json_chat_history = init_json_chat_history()\n",
    "prompt = \"I play my bishop, just not sure where\"\n",
    "json_chat_history, formatted_chat_history = build_chat_history(prompt, json_chat_history, bot=0)\n",
    "full_prompt = build_prompt(prompt, formatted_chat_history, \"classifier\")\n",
    "output = local_llm(full_prompt)\n",
    "json_chat_history, formatted_chat_history = build_chat_history(output, json_chat_history, bot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opponent: I play my bishop, just not sure where\n",
      "Carlus Magnusen: \n",
      "{\"classification\": \"chess move\"}\n"
     ]
    }
   ],
   "source": [
    "print(formatted_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opponent: I play my bishop, just not sure where\n",
      "Carlus Magnusen: {\"classification\": \"chess move\"}\n"
     ]
    }
   ],
   "source": [
    "print(formatted_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {'formatted_chat_history', 'prompt'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/chess_bot.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/chess_bot.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m prompt \u001b[39m=\u001b[39m PromptTemplate(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/chess_bot.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m#input_variables=[\"human_input\", \"history\"], \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/chess_bot.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     input_variables\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mhuman_input\u001b[39m\u001b[39m\"\u001b[39m], \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/chess_bot.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     template\u001b[39m=\u001b[39mtemplate\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/chess_bot.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/chess_bot.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m simple_chain \u001b[39m=\u001b[39m LLMChain(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/chess_bot.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     llm\u001b[39m=\u001b[39mlocal_llm, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/chess_bot.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     prompt\u001b[39m=\u001b[39mprompt, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/chess_bot.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/chess_bot.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m#memory=ConversationBufferWindowMemory(k=2),\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/chess_bot.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/chess_bot.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m output \u001b[39m=\u001b[39m simple_chain\u001b[39m.\u001b[39;49mpredict(human_input\u001b[39m=\u001b[39;49mprompt)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/chess_bot.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m chat_history, formatted_chat_history \u001b[39m=\u001b[39m build_chat_history(output, chat_history, bot\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/chess_bot/lib/python3.11/site-packages/langchain/chains/llm.py:257\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    243\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/anaconda3/envs/chess_bot/lib/python3.11/site-packages/langchain/chains/base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    251\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    252\u001b[0m     inputs: Union[Dict[\u001b[39mstr\u001b[39m, Any], Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m     include_run_info: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    260\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    261\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[1;32m    263\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39m            `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_inputs(inputs)\n\u001b[1;32m    287\u001b[0m     callback_manager \u001b[39m=\u001b[39m CallbackManager\u001b[39m.\u001b[39mconfigure(\n\u001b[1;32m    288\u001b[0m         callbacks,\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata,\n\u001b[1;32m    295\u001b[0m     )\n\u001b[1;32m    296\u001b[0m     new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/chess_bot/lib/python3.11/site-packages/langchain/chains/base.py:443\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    441\u001b[0m     external_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mload_memory_variables(inputs)\n\u001b[1;32m    442\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mexternal_context)\n\u001b[0;32m--> 443\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_inputs(inputs)\n\u001b[1;32m    444\u001b[0m \u001b[39mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/anaconda3/envs/chess_bot/lib/python3.11/site-packages/langchain/chains/base.py:195\u001b[0m, in \u001b[0;36mChain._validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    193\u001b[0m missing_keys \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_keys)\u001b[39m.\u001b[39mdifference(inputs)\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m missing_keys:\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing some input keys: \u001b[39m\u001b[39m{\u001b[39;00mmissing_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'formatted_chat_history', 'prompt'}"
     ]
    }
   ],
   "source": [
    "chat_history, formatted_chat_history = build_chat_history(prompt, chat_history, bot=0)\n",
    "\n",
    "template = build_prompt(prompt, formatted_chat_history)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    #input_variables=[\"human_input\", \"history\"], \n",
    "    input_variables=[\"human_input\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "simple_chain = LLMChain(\n",
    "    llm=local_llm, \n",
    "    prompt=prompt, \n",
    "    verbose=True, \n",
    "    #memory=ConversationBufferWindowMemory(k=2),\n",
    ")\n",
    "\n",
    "output = simple_chain.predict(human_input=prompt)\n",
    "\n",
    "chat_history, formatted_chat_history = build_chat_history(output, chat_history, bot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Opponent': [\"I'm Carlus Magnusen\"],\n",
       " 'Carlus Magnusen': [\"I'm Carlus Magnusen\", \"I'm Carlus Magnusen\"]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opponent: I'm Carlus Magnusen\n",
      "Carlus Magnusen: I'm Carlus Magnusen\n",
      "Carlus Magnusen: I'm Carlus Magnusen\n"
     ]
    }
   ],
   "source": [
    "print(formatted_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(prompt, chat_history):\n",
    "\n",
    "formated_chat_history = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "    template = \"\"\"[INST] <<SYS>>\n",
    "You are a Carlus Magnusen, the best chess player in the known universe and your purpose is to play chess with whoever wants to play with you. You are very confident and arrogant. You trash talk your opponents when you play chess. Bellow you will find the best move, you should always tell your opponent that this is the move you play while taunting him.\n",
    "Best Move: e6\n",
    "<</SYS>>\n",
    "{chat_history}\n",
    "Opponent: {prompt}[/INST]\n",
    "Carlus Magnusen: \n",
    "\"\"\"\n",
    "\n",
    "return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = prompt_chatbot(\"hi\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Opponent': ['hi', 'hi']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Opponent\" not in chat_history:\n",
    "    chat_history[\"Opponent\"] = \"Hello, how can I help you?\"\n",
    "else:\n",
    "    chat_history[\"Opponent\"] = \"I need assistance with a different issue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_config = {\n",
    "    \"Debug Log File\": \"\",\n",
    "    \"Contempt\": 0,\n",
    "    \"Min Split Depth\": 0,\n",
    "    \"Threads\": 8, # More threads will make the engine stronger, but should be kept at less than the number of logical processors on your computer.\n",
    "    \"Ponder\": \"false\",\n",
    "    \"Hash\": 2048, # Default size is 16 MB. It's recommended that you increase this value, but keep it as some power of 2. E.g., if you're fine using 2 GB of RAM, set Hash to 2048 (11th power of 2).\n",
    "    \"MultiPV\": 1,\n",
    "    \"Skill Level\": 20,\n",
    "    \"Move Overhead\": 10,\n",
    "    \"Minimum Thinking Time\": 20,\n",
    "    \"Slow Mover\": 100,\n",
    "    \"UCI_Chess960\": \"false\",\n",
    "    \"UCI_LimitStrength\": \"false\",\n",
    "    \"UCI_Elo\": 9999\n",
    "}\n",
    "\n",
    "Stockfish(path=\"/mnt/c/Users/Antonio/Desktop/Projects/chess_bot/stockfish/stockfish-windows-x86-64-avx2.exe\",  depth=22, parameters=sf_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
